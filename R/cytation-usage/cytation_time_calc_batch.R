# cytation_time_calc_batch.R
# R script to calculate active scan time from Cytation data audit trail
# Theresa Swayne, 2019-2021
# Reads a folder of TXT files generated by the Data Audit Trail in Gen5 v. 2
# Note -- include only the audit trail. No procedure summary or other info.
# Date and time use the default Gen5 format.

# Caveat -- output contains multiple lines per protocol. Use the last line of each one.
# TODO: eliminate this behavior by either 
#      selecting unique reads by start time, or 
#      removing the beginning of each text file name (since these are not unique by protocol), or
#      eliminating all but the last read per protocol (the one with most lines)
# TODO: how to distinguish multiple plates with same protocol vs multiple runs of same plate? File suffix?

# ---- Setup ----

require(tidyverse) # for reading and parsing
require(tcltk) # for file choosing

# ---- User chooses the input folder ----

logfolder <- tk_choose.dir(default = "", caption = "OPEN the folder with log data") # prompt user

# Get a list of files in the folder

files <- dir(logfolder, pattern = "*.txt") 

# Read the data -- parsing errors may occur but data should be read ok.

mergedDataWithNames <- tibble(filename = files) %>% # column 1 = file names
  mutate(file_contents =
           map(filename,          # column 2 = all the data in the file
               ~ read_csv(file.path(logfolder, .),
                          col_types = cols(Comment = col_character(), 
                                           Date = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")), 
                          skip = 2)))


# make the list into a flat file -- each row contains its source filename

logdata <- unnest(mergedDataWithNames, cols=c(file_contents)) # added in response to 'cols is now required' error


# ---- Find start and end times ----

# sort by filename and then date; since merged data is not automatically in order.
arrange(logdata, filename, Date)

# remove lines representing duplication of the same read, 
# which are present in multiple files
logdata_unique <- distinct(logdata, Date, .keep_all = TRUE)

# add a column for the Read number to help with merging the data later 
# read numbers are consecutive through all the log files

startTimes <- logdata_unique %>% 
  filter(grepl("started", Event)) %>%
  select(filename, Start = Date)  %>%
  mutate(Read = row_number())

endTimes <- logdata_unique %>% 
  filter(grepl("completed", Event)) %>%
  select(filename, End = Date)  %>%
  mutate(Read = row_number())

# combine start and end times so each row represents a single read step

scanTimes <- full_join(startTimes, endTimes, by = "Read") %>% # combine using original read number
  mutate(filename = filename.x) %>% # get rid of duplicate columns
  select(-c(filename.y, filename.x))

# ---- Calculate elapsed time ----

scanTimes <- scanTimes %>% 
  mutate(elapsedTime = difftime(End, Start, units = "hours")) # convert elapsedTime to hours

# calculate active time for each experiment (represented by 1 unique filename suffix)

# create a column representing the filename after the timestamp characters

scanTimes <- scanTimes %>% 
  mutate(Expt = substring(filename, 15))

# filter out NAs that arise from aborted runs

scanTimeTotal <-  scanTimes %>%
  group_by(Expt) %>%
  filter(is.na(elapsedTime) == FALSE) %>%
  summarise(TotalHours = sum(elapsedTime)) 

# ---- Output ----

# save results in the parent of the log directory 

parentName <- basename(dirname(logfolder)) # name of the log directory without higher levels

parentDir <- dirname(logfolder) # parent of the log directory

outputFile = paste(Sys.Date(), basename(logfolder), "_totals.csv") # spaces will be inserted

write_csv(scanTimeTotal,file.path(parentDir, outputFile))

