# cytation_time_calc_batch.R
# R script to calculate active scan time from Cytation data audit trail
# Theresa Swayne, 2022
# Reads a folder of TXT files generated by the Data Audit Trail in Gen5
# Note -- include only the audit trail. No procedure summary or other info.
# Date and time use the default Gen5 format "10/1/2021 2:49:43 PM"

# Caveat -- Check for errors in the text files. Aborted reads can result in incorrect calculations.

# ---- Setup ----

require(tidyverse) # for reading and parsing
require(tcltk) # for file choosing

# ---- User chooses the input folder ----

logfolder <- tk_choose.dir(default = "", caption = "OPEN the folder with log data") # prompt user

# Get a list of files in the folder

files <- dir(logfolder, pattern = "*.txt") 

# Read the data -- parsing errors may occur but data should be read ok.

mergedDataWithNames <- tibble(filename = files) %>% # column 1 = file names
  mutate(file_contents =
           map(filename,          # column 2 = all the data in the file
               ~ read_csv(file.path(logfolder, .),
                          col_types = cols(Comment = col_character(), 
                                           Date = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")), 
                          skip = 2)))


# make the list into a flat file -- each row contains its source filename

logdata <- unnest(mergedDataWithNames, cols=c(file_contents)) # added in response to 'cols is now required' error


# ---- Find start and end times ----

# remove lines representing duplication of the same read, 
# which are present in multiple files
logdata_unique <- distinct(logdata, Date, .keep_all = TRUE)

# create a column representing the filename after the timestamp characters
logdata_unique <- logdata_unique %>% 
  mutate(Expt = substring(filename, 15))

# sort in order of experiment then read; since merged data is not automatically in order.
arrange(logdata_unique, Expt, filename, Date)

# add a column for the Read number to help with merging the data later 
# read numbers are consecutive through all the log files

startTimes <- logdata_unique %>% 
  filter(grepl("started", Event)) %>%
  select(filename, Start = Date, Expt)  %>%
  mutate(Read = row_number())

endTimes <- logdata_unique %>% 
  filter(grepl("completed", Event)) %>%
  select(filename, End = Date)  %>%
  mutate(Read = row_number())

# combine start and end times so each row represents a single read step

scanTimes <- full_join(startTimes, endTimes, by = "Read") %>% # combine using original read number
  mutate(filename = filename.x) %>% # get rid of duplicate columns
  select(-c(filename.y, filename.x))

# ---- Calculate elapsed time ----

scanTimes <- scanTimes %>% 
  mutate(elapsedTime = difftime(End, Start, units = "hours")) # convert elapsedTime to hours

# calculate active time for each experiment (represented by 1 unique filename suffix)

# Sort by expt and start time

scanTimes <- scanTimes %>% 
  arrange(Expt, Start)

# filter out NAs that arise from aborted runs

scanTimeTotal <-  scanTimes %>%
  group_by(Expt) %>%
  filter(is.na(elapsedTime) == FALSE) %>%
  summarise(TotalHours = sum(elapsedTime)) 

# ---- Output ----

# save results in the parent of the log directory 

parentName <- basename(dirname(logfolder)) # name of the log directory without higher levels

parentDir <- dirname(logfolder) # parent of the log directory

outputFileTotal = paste(Sys.Date(), basename(logfolder), "_totals.csv") # spaces will be inserted
outputFile = paste(Sys.Date(), basename(logfolder), "_times.csv") # spaces will be inserted

write_csv(scanTimeTotal,file.path(parentDir, outputFileTotal))
write_csv(scanTimes,file.path(parentDir, outputFile))

